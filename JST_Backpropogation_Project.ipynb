{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEBNxguB5vp9"
      },
      "source": [
        "#Prediksi Pendapatan Penduduk Menggunakan Metode Gaussian Naive Bayes Classifier\n",
        "\n",
        "Bernadus Dwii Adicitra_215314052\n",
        "\n",
        "Eustachia Agnesti Marta Da Silva_215314085\n",
        "\n",
        "Lusia Juliana Silaban_215314087\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIuFFMsCo-RJ"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing\n",
        "import matplotlib.pyplot as plt # for data visualization purposes\n",
        "import seaborn as sns # for statistical data visualization\n",
        "%matplotlib inline\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DWegRpf5teq"
      },
      "outputs": [],
      "source": [
        "# Import dataset\n",
        "data = '/content/drive/MyDrive/Colab Notebooks/Examples/adult.csv'\n",
        "\n",
        "df = pd.read_csv(data, header=None, sep=',\\s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXrQhhje6-lf"
      },
      "source": [
        "#  Step 1 - Eksploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R00tlqZT6-GM"
      },
      "outputs": [],
      "source": [
        "# View dimensions of dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHhAnaS95tbY"
      },
      "outputs": [],
      "source": [
        "# View top 5 rows of dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFSNYLsN7GFa"
      },
      "source": [
        "#Rename column names\n",
        "We can see that the dataset does not have proper column names. The columns are merely labelled as 0,1,2.... and so on. We should give proper names to the columns. I will do it as follows:-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzPDm7bY5tYq"
      },
      "outputs": [],
      "source": [
        "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n",
        "             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
        "\n",
        "df.columns = col_names\n",
        "\n",
        "# df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVf9e6jQ5tV1"
      },
      "outputs": [],
      "source": [
        "# let's again preview the dataset\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wk7365SF5tS7"
      },
      "outputs": [],
      "source": [
        "# view summary of dataset\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FZBRYkd7QN9"
      },
      "source": [
        "# a) Get descriptive atatistics for numerical and categorical features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXzSY3GH5tEn"
      },
      "outputs": [],
      "source": [
        "# Get descriptive statistics for numerical and categorical features.\n",
        "print(df.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKqxwgWC7ZCS"
      },
      "source": [
        "# b) Chek categorical of variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k2G7iJk7SZN"
      },
      "outputs": [],
      "source": [
        "# find categorical variables\n",
        "\n",
        "categorical = [var for var in df.columns if df[var].dtype=='O']\n",
        "\n",
        "print('There are {} categorical variables\\n'.format(len(categorical)))\n",
        "\n",
        "print('The categorical variables are :\\n\\n', categorical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wxNbkuV7SV8"
      },
      "outputs": [],
      "source": [
        "# view the categorical variables\n",
        "\n",
        "df[categorical].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVwXPZOo7SS8"
      },
      "outputs": [],
      "source": [
        "# check missing values in categorical variables\n",
        "\n",
        "df[categorical].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tMlvlTA7pdP"
      },
      "source": [
        "We can see that there are no missing values in the categorical variables. I will confirm this further."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANfIkm_57SP0"
      },
      "outputs": [],
      "source": [
        "# view frequency counts of values in categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(df[var].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-Cofap77xYc"
      },
      "source": [
        "# c) replace '?' values in occupation variable with `NaN`\n",
        "\n",
        "Now, we can see that there are no values encoded as ? in the workclass variable.\n",
        "\n",
        "I will adopt similar approach with occupation and native_country column."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# replace '?' values in 'df' with `NaN`\n",
        "\n",
        "df = df.replace('?', np.nan)"
      ],
      "metadata": {
        "id": "UMlXbXwUHiz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC_pyjVq7tm7"
      },
      "outputs": [],
      "source": [
        "# again view frequency counts of values in categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(df[var].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1eyw_GY8GMk"
      },
      "source": [
        "# d) Number of labels: cardinality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-EQOAhW7tkI"
      },
      "outputs": [],
      "source": [
        "# check for cardinality in categorical variables\n",
        "\n",
        "for var in categorical:\n",
        "\n",
        "    print(var, 'contains', len(df[var].unique()), 'labels')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLK98iZ78cNX"
      },
      "source": [
        "# e) Category distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4j-4N5kK7th9"
      },
      "outputs": [],
      "source": [
        "# Calculate and visualise the category distribution for each categorical feature with diagram\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.countplot(data=df, x=feature)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXo05hmg81Kt"
      },
      "outputs": [],
      "source": [
        "# Calculate and visualise the category distribution for each categorical feature with boxplot\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(data=df, x=feature)\n",
        "    plt.title(f'Distribution of {feature}')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0io2c7b69Fyw"
      },
      "outputs": [],
      "source": [
        "# sns.pairplot(df, hue='income')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWbGunOa9Gq9"
      },
      "outputs": [],
      "source": [
        "# Calculate and visualise the category distribution for 'income' with diagram\n",
        "for feature in categorical_features:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.countplot(data=df, x=feature, hue='income')\n",
        "    plt.title(f'{feature} vs Income')\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfIxI2dP9LiS"
      },
      "source": [
        "# f) Numerical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRvXGtPv9IhX"
      },
      "outputs": [],
      "source": [
        "# find numerical variables\n",
        "\n",
        "numerical = [var for var in df.columns if df[var].dtype!='O']\n",
        "\n",
        "print('There are {} numerical variables\\n'.format(len(numerical)))\n",
        "\n",
        "print('The numerical variables are :', numerical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zj6F7YK9IXx"
      },
      "outputs": [],
      "source": [
        "# view the numerical variables\n",
        "\n",
        "df[numerical].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fI6Ghd719IUY"
      },
      "outputs": [],
      "source": [
        "# check missing values in numerical variables\n",
        "\n",
        "df[numerical].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7vltdBp9T84"
      },
      "source": [
        "# Step 3 - Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgEeiH7k9XdA"
      },
      "source": [
        "# a) Handling missing value\n",
        "\n",
        "After changing the '?' above to NaN, now the categorical data will have a null value (missing value). Let's clean up the missing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pXRIz6b9ISC"
      },
      "outputs": [],
      "source": [
        "# Check missing values in categorical variables again\n",
        "df[categorical].isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxV528PX9o6r"
      },
      "outputs": [],
      "source": [
        "# print categorical variables with missing data\n",
        "\n",
        "for col in categorical:\n",
        "    if df[col].isnull().mean()>0:\n",
        "        print(col, (df[col].isnull().mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOhZ2NgG9qkc"
      },
      "outputs": [],
      "source": [
        "# impute missing categorical variables with most frequent value\n",
        "\n",
        "for df2 in [df]:\n",
        "    df2['workclass'].fillna(df['workclass'].mode()[0], inplace=True)\n",
        "    df2['occupation'].fillna(df['occupation'].mode()[0], inplace=True)\n",
        "    df2['native_country'].fillna(df['native_country'].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3djAX3qF9shB"
      },
      "outputs": [],
      "source": [
        "# again check missing values in categorical variables again\n",
        "df[categorical].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0TNrBMU9u6n"
      },
      "source": [
        "There's no NaN now. And let's go do the next step ^_^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d2yfJuE9x76"
      },
      "source": [
        "# b) Encoder categorial variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glUEX57h9sdx"
      },
      "outputs": [],
      "source": [
        "!pip install category_encoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fef7Yt7g9sbL"
      },
      "outputs": [],
      "source": [
        "# define the columns you want to encode\n",
        "category_column = ['workclass', 'education', 'marital_status', 'occupation',\n",
        "                   'relationship','race', 'sex', 'native_country']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gieTD5cn93Wg"
      },
      "source": [
        "1. One-Hot Encoding: Converts each category into a binary column (0 or 1). This is a commonly used method and works well for linear models and many machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN4m9Ndr9sY4"
      },
      "outputs": [],
      "source": [
        "# encode remaining variables with One-Hot Encoding\n",
        "import category_encoders as ce\n",
        "\n",
        "encoder = ce.OneHotEncoder(cols=category_column)\n",
        "\n",
        "data1 = encoder.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOrzgGuk9sVs"
      },
      "outputs": [],
      "source": [
        "data1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIOsI3x29sTC"
      },
      "outputs": [],
      "source": [
        "data1.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRIibw8C9-Xs"
      },
      "source": [
        "After one-hot encoding, there are 106 columns. It seems too much now :)\n",
        "\n",
        "Let's try another encoding..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdZhfckp-DdR"
      },
      "source": [
        "2. Binary Encoding: Converts categories into binary representations, and then combines the bits into numerical features. This reduces the number of columns created compared to one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLwuMfpi-A1T"
      },
      "outputs": [],
      "source": [
        "# encode remaining variables with Binary Encoding\n",
        "from category_encoders import BinaryEncoder\n",
        "\n",
        "binary_encoder = BinaryEncoder(cols=category_column)\n",
        "data2 = binary_encoder.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC9Z5o8P-Bp5"
      },
      "outputs": [],
      "source": [
        "data2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S8hyM5I-Bml"
      },
      "outputs": [],
      "source": [
        "data2.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4aCjVwBjnTK"
      },
      "source": [
        "After one-hot encoding, there are 37 columns. It is still too much :)\n",
        "\n",
        "Let's try another encoding..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nTCa_tejwUk"
      },
      "source": [
        "3. Label Encoder: Label encoding is a common technique used in machine learning to convert categorical variables into numerical representations. It is particularly useful when working with algorithms that cannot directly handle categorical data. However, when dealing with datasets that have multiple columns containing categorical variables, applying label encoding individually to each column can be time-consuming and error-prone. In such cases, a multi-column label encoding approach can be employed to streamline the process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx-cBWwcBKgF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Initialitation LabelEncoder\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "\n",
        "# create a copy of data3 to df\n",
        "data3 = df.copy()\n",
        "\n",
        "data3 = data3.apply(lambda col: label_encoder.fit_transform(col) if col.dtype == 'object' else col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t50xCqMeCENd"
      },
      "outputs": [],
      "source": [
        "data3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_hmOAtnCF6B"
      },
      "outputs": [],
      "source": [
        "data3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biAsFUrP-Bj_"
      },
      "outputs": [],
      "source": [
        "data3.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMNvbvm1-mB7"
      },
      "source": [
        "Now we have a numeric dataset with 15 columns after encoder. We will use this dataset (data3) to build the model classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1WM4BIGs3HI"
      },
      "source": [
        "# c) Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCHqMQ0ykpe0"
      },
      "outputs": [],
      "source": [
        "# Korelasi setiap fitur\n",
        "correlations = data3.corr()\n",
        "print(correlations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIV1xcbpk3tF"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(data3.corr(), annot=True, cmap='RdYlGn', linewidths=0.5)\n",
        "# data.corr() ---> correlation matrix\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(20,20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIkAMGmG-nIr"
      },
      "source": [
        "# Step 4 -Split data into separate training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBDTZJ7a-BhJ"
      },
      "outputs": [],
      "source": [
        "# Split data for training and test set\n",
        "\n",
        "X = data3.drop(['income'], axis=1)\n",
        "\n",
        "y = data3['income']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9O8sxvH6w1-"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "# Seleksi fitur menggunakan Chi-squared test\n",
        "selector = SelectKBest(chi2, k=2)\n",
        "X_new = selector.fit_transform(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzVRoYhn-Bea"
      },
      "outputs": [],
      "source": [
        "# split X and y into training and testing sets\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Fold Cross Validation"
      ],
      "metadata": {
        "id": "wT_BH-xlIX5P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vohj5B3TbGBI"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# # Membuat objek KFold dengan 5 lipatan\n",
        "# kf = KFold(n_splits=5)\n",
        "\n",
        "# for train_index, test_index in kf.split(X):\n",
        "#     X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "#     y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZQmlq-1-4OF"
      },
      "source": [
        "# Step 5 - Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpCYaaHo-BcE"
      },
      "outputs": [],
      "source": [
        "# check the shape of X_train and X_test\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8-_kYD-tIFD"
      },
      "source": [
        "# a) Reduksi Dimensi: Latent Discriminant  Analysis (LDA)\n",
        "\n",
        "Latent Dirichlet Allocation is a probabilistic model used for topic modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvhnLnSUw9h9"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "#Fit the LDA model\n",
        "model = LinearDiscriminantAnalysis()\n",
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vbMVDWu-6_T"
      },
      "outputs": [],
      "source": [
        "# Chek class/category balance\n",
        "print(df['income'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJR6xw6Q_CsH"
      },
      "source": [
        "The target class shows an imbalance in the income data of people <=50k and >50k. The gap in the number of classes is around 3:1. For cases where the classification of these classes is very unbalanced, it is necessary to resample the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9jCJI85--kW"
      },
      "source": [
        "# b) Resampling\n",
        "Oversampling on Minority Classes: Adding copies of the minority class examples to balance the dataset. A frequently used technique is SMOTE (Synthetic Minority Over-sampling Technique). Undersampling the Majority Class: Reduces the number of examples from the majority class to balance the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GgP0NAA-68A"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "\n",
        "# Oversampling\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Undersampling on the oversampled data\n",
        "undersampler = RandomUnderSampler()\n",
        "X_resampled, y_resampled = undersampler.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "# Combining oversampling and undersampling on the undersampled data\n",
        "smote_enn = SMOTEENN()\n",
        "X_resampled, y_resampled = smote_enn.fit_resample(X_resampled, y_resampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmuBid9e-66N"
      },
      "outputs": [],
      "source": [
        "# print the count of each class in the resampled data\n",
        "print(y_resampled.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3NgpGro_YYo"
      },
      "source": [
        "Although the number of samples between these two classes is not exactly the same, the difference is not very significant. Therefore, this data can be considered fairly balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD_hjaHg_bWa"
      },
      "source": [
        "# Step 6 - Feature Scaling\n",
        "We use RobustScaler from sklearn.preprocessing to perform feature scaling. This scaling uses a method that is more robust to outliers than standard scaling. RobustScaler removes the median and scales the data by quartiles (IQR: Q3 - Q1). This transformation transforms each feature into the same range and is important for some machine learning algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GiHnYPg_ae-"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnOK4ZLN-64U"
      },
      "outputs": [],
      "source": [
        "cols = X_train.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj8V4XUW-62O"
      },
      "outputs": [],
      "source": [
        "# Feature scalling\n",
        "\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "scaler = RobustScaler()\n",
        "\n",
        "# Transformasi Data\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DbJRj0D-6z5"
      },
      "outputs": [],
      "source": [
        "# Konvertion to DataFrame\n",
        "\n",
        "X_train = pd.DataFrame(X_train, columns=[cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fHgCO6D-6xm"
      },
      "outputs": [],
      "source": [
        "# Konvertion to DataFrame\n",
        "\n",
        "X_test = pd.DataFrame(X_test, columns=[cols])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBLwZk2K-6u3"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLUK17hP_rq5"
      },
      "source": [
        "We now have X_train dataset ready to be fed into the Gaussian Naive Bayes classifier. Let's do it as follows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2g289Hw_toc"
      },
      "source": [
        "# Step 7 - Modelling Backpropogation to predict the adult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rppOYYKE_6e5"
      },
      "source": [
        "# 1 Menggunakan splitting data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k320fYVb8fx6"
      },
      "source": [
        "Now in the next step, we have to start initializing the hyperparameters. We will input the learning rate, iterations, input size, number of hidden layers, and number of output layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7nToSHmi8hS"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Mean squared error function\n",
        "def mean_squared_error(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    return np.mean(y_pred_classes == y_true_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nUV1xsvjQCk"
      },
      "outputs": [],
      "source": [
        "# Model parameter\n",
        "output_size = 2  # class label\n",
        "input_size = X_train.shape[1]  # The number of features\n",
        "hidden_size = 64  # Hidden layer size\n",
        "learning_rate = 0.1\n",
        "iterations = 5000\n",
        "N = X_train.shape[0]  # The number of sampel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PCUbrSEjV33"
      },
      "outputs": [],
      "source": [
        "# Weights initiatilation (W)\n",
        "np.random.seed(10)\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "\n",
        "# One-hot encode y_train and y_test\n",
        "y_train_one_hot = np.eye(output_size)[y_train]\n",
        "y_test_one_hot = np.eye(output_size)[y_test]\n",
        "\n",
        "# DataFrame to save results of model traning\n",
        "results = pd.DataFrame(columns=[\"mse\", \"accuracy\"])\n",
        "\n",
        "# List to store the prediction result and the actual label\n",
        "# prediction = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvGuNq0yjZYQ"
      },
      "outputs": [],
      "source": [
        "# The training loop\n",
        "for itr in range(iterations):\n",
        "    # Feedforward propagation\n",
        "    Z1 = np.dot(X_train, W1)\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2)\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    # Calculate error\n",
        "    mse = mean_squared_error(A2, y_train_one_hot)\n",
        "    acc = accuracy(y_train_one_hot, A2)\n",
        "    new_row = pd.DataFrame({\"mse\": [mse], \"accuracy\": [acc]})\n",
        "    results = pd.concat([results, new_row], ignore_index=True)\n",
        "\n",
        "    # Backpropagation\n",
        "    E1 = A2 - y_train_one_hot\n",
        "    dW1 = E1 * A2 * (1 - A2)\n",
        "    E2 = np.dot(dW1, W2.T)\n",
        "    dW2 = E2 * A1 * (1 - A1)\n",
        "\n",
        "    # Update weights\n",
        "    W2_update = np.dot(A1.T, dW1) / N\n",
        "    W1_update = np.dot(X_train.T, dW2) / N\n",
        "    W2 = W2 - learning_rate * W2_update\n",
        "    W1 = W1 - learning_rate * W1_update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGlmnch_jf7y"
      },
      "outputs": [],
      "source": [
        "# Display training results\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS-CyfNAiH1u"
      },
      "outputs": [],
      "source": [
        "# Visualizing the results\n",
        "\n",
        "results.mse.plot(title=\"Mean Squared Error\")\n",
        "plt.show()\n",
        "\n",
        "results.accuracy.plot(title=\"Accuracy\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHzZ108WBZzX"
      },
      "outputs": [],
      "source": [
        "# Test the model\n",
        "Z1 = np.dot(X_test, W1)\n",
        "A1 = sigmoid(Z1)\n",
        "Z2 = np.dot(A1, W2)\n",
        "A2 = sigmoid(Z2)\n",
        "test_acc = accuracy(np.eye(output_size)[y_test], A2)\n",
        "print(\"Test accuracy: {}\".format(test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "aO9bvx881okH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on test data\n",
        "Z1_test = np.dot(X_test, W1)\n",
        "A1_test = sigmoid(Z1_test)\n",
        "Z2_test = np.dot(A1_test, W2)\n",
        "A2_test = sigmoid(Z2_test)"
      ],
      "metadata": {
        "id": "daY0UmO-pjnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse_test = mean_squared_error(A2_test, y_test_one_hot)\n",
        "acc_test = accuracy(y_test_one_hot, A2_test)\n",
        "\n",
        "# Calculating other evaluation metrics\n",
        "y_pred_classes = np.argmax(A2_test, axis=1)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "precision = precision_score(y_test, y_pred_classes, average='macro')\n",
        "recall = recall_score(y_test, y_pred_classes, average='macro')\n",
        "f1 = f1_score(y_test, y_pred_classes, average='macro')\n",
        "\n",
        "print(f\"Test MSE: {mse_test}\")\n",
        "print(f\"Test Accuracy: {acc_test}\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1 Score: {f1}\")"
      ],
      "metadata": {
        "id": "vK4Zbp_3oHvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "_cGGXOaPzoKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Viewing confusion matrix\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_uxHpOufzyDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhWaQsoYieWg"
      },
      "source": [
        "# 2. Menggunakan K-Fold Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os9wZoj4id1k"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-a_K7atilHZ"
      },
      "outputs": [],
      "source": [
        "# Sigmoid activation function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Mean squared error function\n",
        "def mean_squared_error(y_pred, y_true):\n",
        "    return np.mean((y_pred - y_true) ** 2)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def accuracy(y_true, y_pred):\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true_classes = np.argmax(y_true, axis=1)\n",
        "    return np.mean(y_pred_classes == y_true_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP7qyL_zilEb"
      },
      "outputs": [],
      "source": [
        "# Model hyperparamter\n",
        "output_size = 2  # Count of class\n",
        "hidden_size = 64  # Hidden layer size (neuron)\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "k_folds = 5  # Count of fold for k-fold cross-validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpjLktP-ik-d"
      },
      "outputs": [],
      "source": [
        "# k-fold initialitation\n",
        "kf = KFold(n_splits=k_folds)\n",
        "\n",
        "# DataFrame to save results of model traning\n",
        "results = pd.DataFrame(columns=[\"fold\", \"mse\", \"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9vACmMq1ik7J"
      },
      "outputs": [],
      "source": [
        "# Loop k-fold cross-validation\n",
        "fold_idx = 1\n",
        "\n",
        "# Make objek KFold with 3 cv\n",
        "kf = KFold(n_splits=3)\n",
        "\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "    # One-hot encode y_train and y_test\n",
        "    y_train_one_hot = np.eye(output_size)[y_train]\n",
        "    y_test_one_hot = np.eye(output_size)[y_test]\n",
        "\n",
        "    # Weights initialitation\n",
        "    np.random.seed(42)\n",
        "    W1 = np.random.randn(X_train.shape[1], hidden_size)\n",
        "    W2 = np.random.randn(hidden_size, output_size)\n",
        "\n",
        "    # Loop pelatihan\n",
        "    for itr in range(iterations):\n",
        "        # Feedforward propagation\n",
        "        Z1 = np.dot(X_train, W1)\n",
        "        A1 = sigmoid(Z1)\n",
        "        Z2 = np.dot(A1, W2)\n",
        "        A2 = sigmoid(Z2)\n",
        "\n",
        "        # Calculate error\n",
        "        mse = mean_squared_error(A2, y_train_one_hot)\n",
        "        acc = accuracy(y_train_one_hot, A2)\n",
        "\n",
        "        # Backpropagation\n",
        "        E1 = A2 - y_train_one_hot\n",
        "        dW1 = E1 * A2 * (1 - A2)\n",
        "        E2 = np.dot(dW1, W2.T)\n",
        "        dW2 = E2 * A1 * (1 - A1)\n",
        "\n",
        "        # Update weights\n",
        "        W2_update = np.dot(A1.T, dW1) / X_train.shape[0]\n",
        "        W1_update = np.dot(X_train.T, dW2) / X_train.shape[0]\n",
        "        W2 = W2 - learning_rate * W2_update\n",
        "        W1 = W1 - learning_rate * W1_update\n",
        "\n",
        "    # Evaluation on test data\n",
        "    Z1_test = np.dot(X_test, W1)\n",
        "    A1_test = sigmoid(Z1_test)\n",
        "    Z2_test = np.dot(A1_test, W2)\n",
        "    A2_test = sigmoid(Z2_test)\n",
        "\n",
        "    mse_test = mean_squared_error(A2_test, y_test_one_hot)\n",
        "    acc_test = accuracy(y_test_one_hot, A2_test)\n",
        "\n",
        "    new_row = pd.DataFrame({\"fold\": [fold_idx], \"mse\": [mse_test], \"accuracy\": [acc_test]})\n",
        "    results = pd.concat([results, new_row], ignore_index=True)\n",
        "\n",
        "    fold_idx += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqh5TSJhjEQJ"
      },
      "outputs": [],
      "source": [
        "# Display training results of all cv\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}