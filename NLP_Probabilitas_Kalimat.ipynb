{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lBbH-jCYr2pJXZyzJrhrODonuOTlp-lz",
      "authorship_tag": "ABX9TyPf0VlJfWyNSixQo2dXRKZ9"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mengimport library\n",
        "import nltk\n",
        "import re\n",
        "from nltk.util import ngrams\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2PDaIrscYE-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengunduh resource NLTK yang diperlukan\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "ZRLZlcwVaPcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizer dan stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('indonesian'))"
      ],
      "metadata": {
        "id": "c-ugaS_aaZxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membaca kalimat dari file edukasi26.txt\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/NLP/edukasi26.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "    corpus = file.read()\n",
        "    print(corpus)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YmNsh5lXZ1uf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# melakukan tokenisasi atau pemisahan atar kalimat\n",
        "corpus = sent_tokenize(corpus)\n",
        "corpus"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T_ClG2Jmf8_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    # Menghapus angka dan karakter khusus\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "NHlxx1gibIHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus karakter yang tidak penting pada corpus\n",
        "cleanCorpus = [preprocess_text(sent) for sent in corpus]\n",
        "cleanCorpus"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZrtwAKHkiF4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize dan bersihkan corpus\n",
        "tokens = [['<s>']+nltk.word_tokenize(sent)+['</s>'] for sent in cleanCorpus]\n",
        "tokens"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZL3VV55jZ22u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghitung jumlah token maksimal dari kalimat yang akan diproses\n",
        "max_tokens = max(len(sentence) for sentence in tokens)"
      ],
      "metadata": {
        "id": "kuQ-powBl6GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat pasangan kata dari 1 hingga max_tokens (jumlah token terbanyak dari kalimat yg akan di proses)\n",
        "list_ngrams = [Counter() for _ in range(max_tokens)]\n",
        "for sentence in tokens:\n",
        "    for i in range(max_tokens):\n",
        "        list_ngrams[i].update(ngrams(sentence, i + 1))"
      ],
      "metadata": {
        "id": "IuJfG51Il2G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat fungsi interpolasi untuk menghitung nilai probabilitas\n",
        "def interpolate(checker_words):\n",
        "    temp = 0.0\n",
        "    lambda_value = 0.1  # Inisialisasi nilai lambda\n",
        "    for i in range(len(checker_words)):\n",
        "        ngram = list_ngrams[i]  # Untuk menghitung jumlah pasangan kata\n",
        "        sent = checker_words[len(checker_words) - i - 1:len(checker_words)]  # Menentukan kalimat yang ingin dicari probabilitasnya\n",
        "\n",
        "        if i == 0:\n",
        "            # Menghitung P_ngram untuk unigram\n",
        "            P_ngram = ngram[tuple(sent)] / sum(ngram.values()) if tuple(sent) in ngram else 0\n",
        "        else:\n",
        "            # Digunakan untuk menghitung jumlah pasangan kata tetapi tidak termasuk kata terakhir\n",
        "            ngram_before = list_ngrams[i - 1]\n",
        "            P_ngram = (ngram[tuple(sent)] / ngram_before[tuple(sent[0:len(sent) - 1])]) if tuple(sent) in ngram else 0\n",
        "\n",
        "        if P_ngram == 0:\n",
        "            break  # Jika bagian kecil dari pasangan kata saja nol, stop perhitungan\n",
        "\n",
        "        temp += lambda_value * P_ngram  # Menjumlahkan probabilitas\n",
        "        lambda_value += 0.3  # Menambah nilai lambda\n",
        "\n",
        "    return temp  # Mengembalikan nilai probabilitas"
      ],
      "metadata": {
        "id": "cw5j0zoRbT2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek nilai probabilitas kalimat\n",
        "kalimat1 = \"wisudawan unesa universitas negeri surabaya\"\n",
        "kalimat2 = \"mahasiswa memiliki belasan karya yang terindeks sinta dan scopus\"\n",
        "kalimat3 = \"garap wisudawan mahasiswa sekolah menengah pertama\""
      ],
      "metadata": {
        "id": "JoDtM9AQbT6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencetak probabilitas interpolasi untuk setiap kalimat\n",
        "print(f\"Probabilitas '{kalimat1}': {interpolate(preprocess_text(kalimat1).split()):.10f}\")"
      ],
      "metadata": {
        "id": "saWynzFhbifj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencetak probabilitas interpolasi untuk setiap kalimat\n",
        "print(f\"Probabilitas '{kalimat2}': {interpolate(preprocess_text(kalimat2).split()):.10f}\")"
      ],
      "metadata": {
        "id": "qQW1E5SUbl7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mencetak probabilitas interpolasi untuk setiap kalimat\n",
        "print(f\"Probabilitas '{kalimat3}': {interpolate(preprocess_text(kalimat3).split()):.10f}\")"
      ],
      "metadata": {
        "id": "EGy0MVSFbl-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rCmkWpk_l9iJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}