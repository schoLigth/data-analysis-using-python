{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Ur-WTLTyWce0eLtwT2z4qF7A2lDf3V03",
      "authorship_tag": "ABX9TyM8jgb3qXH9ZiuKBk+i9eA6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 1: Seleksi 1000 Review Unik"
      ],
      "metadata": {
        "id": "wtwb1UAzxGkU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIo2pZKGu5ES"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "def load_data(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Load\n",
        "data_path = \"/content/drive/MyDrive/Colab Notebooks/NLP/priority_3k_labelled.csv\"\n",
        "data = load_data(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat dataset mentah\n",
        "print(data)"
      ],
      "metadata": {
        "id": "HK3KzxK3zMeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleksi 1000 review unik\n",
        "def select_unique_reviews(data, n=1000):\n",
        "    unique_reviews = data.drop_duplicates(subset='review', keep='first')\n",
        "    return unique_reviews.sample(n=min(n, len(unique_reviews)))\n",
        "\n",
        "selected_data = select_unique_reviews(data, 1000)"
      ],
      "metadata": {
        "id": "yr7hx9FjzMiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat dataset sementara\n",
        "print(selected_data)"
      ],
      "metadata": {
        "id": "fXX-F2bKxVgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 2: Preprocessing (Cleaning, Stemming, Tokenizing)"
      ],
      "metadata": {
        "id": "vAtemsJXxiWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "bYhXJJfHyaP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi"
      ],
      "metadata": {
        "id": "wjU4YZrc2Gm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Membuat stemmer untuk bahasa Indonesia\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Preprocessing function untuk bahasa Indonesia\n",
        "def preprocess_text(text):\n",
        "    # Cleaning\n",
        "    text = str(text).lower()  # Mengubah semua teks menjadi huruf kecil\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)  # Hanya menyisakan huruf dan tanda baca ?, !, ., , ¿\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Menghapus spasi ganda\n",
        "    text = re.sub('\\[.*?\\]', '', text)  # Menghapus teks dalam tanda []\n",
        "\n",
        "    # Tokenisasi dan Stemming\n",
        "    tokens = word_tokenize(text)  # Memecah teks menjadi token (kata per kata)\n",
        "    tokens = [stemmer.stem(token) for token in tokens if token not in stopwords.words('indonesian')]  # Stemming dan menghapus stopwords\n",
        "\n",
        "    return ' '.join(tokens)  # Menggabungkan kembali token-token menjadi teks yang bersih\n",
        "\n",
        "# Preprocessing\n",
        "selected_data['cleaned_review'] = selected_data['review'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "sBhu4v_oxVjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat isi dataset setelah processing\n",
        "print(selected_data)"
      ],
      "metadata": {
        "id": "uvtvKyV73M7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Misalkan 'selected_data' adalah DataFrame Anda\n",
        "# Membuat objek LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Mengubah kolom 'category_sentiment' menjadi label angka\n",
        "selected_data['label'] = label_encoder.fit_transform(selected_data['category_sentiment'])\n",
        "\n",
        "# Menampilkan hasil\n",
        "print(selected_data[['category_sentiment', 'label']].head())"
      ],
      "metadata": {
        "id": "S4O5Hkt_Exvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cek nama kolom yang tersedia di dataset\n",
        "print(selected_data.columns)"
      ],
      "metadata": {
        "id": "3IuxjmkO5f3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 3: Split Data"
      ],
      "metadata": {
        "id": "3QyHCANzx1mg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split Data menjadi 800 untuk Training dan 200 untuk Testing\n",
        "def split_data(data, test_size=0.2):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        data['cleaned_review'], data['label'], test_size=test_size, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, y_train, y_test = split_data(selected_data, test_size=0.2)"
      ],
      "metadata": {
        "id": "P_x6o7GNxVnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat data training\n",
        "print(X_train)"
      ],
      "metadata": {
        "id": "2INhJjjW6S1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lihat data target training\n",
        "print(y_train)"
      ],
      "metadata": {
        "id": "GpzA4s0t6S4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.min())"
      ],
      "metadata": {
        "id": "kzmO-JjIFOcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.max())"
      ],
      "metadata": {
        "id": "RfgULd-CFSXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 4: Train Data Menggunakan Naïve Bayes dan Logistic Regression"
      ],
      "metadata": {
        "id": "by1RTZQCx8Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# A. Train Naive Bayes\n",
        "def train_naive_bayes(X_train, y_train):\n",
        "    model = MultinomialNB()       # Membuat instance dari model MultinomialNB\n",
        "    model.fit(X_train, y_train)   # Melatih model Naive Bayes\n",
        "    return model\n",
        "\n",
        "# B. Train Logistic Regression\n",
        "def train_logistic_regression(X_train, y_train):\n",
        "    model = LogisticRegression(max_iter=1000) # Membuat instance dari model LogisticRegression dengan parameter max_iter=1000 untuk menentukan jumlah maksimum iterasi dalam optimasi\n",
        "    model.fit(X_train, y_train)               # Melatih model regresi logistik\n",
        "    return model"
      ],
      "metadata": {
        "id": "Sfj1CnFLx54q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 5: Vector Embedding Menggunakan TF dan TF-IDF"
      ],
      "metadata": {
        "id": "K7TwgOPeyA_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# A. Vector embedding dengan TF\n",
        "def vectorize_tf(X_train, X_test):\n",
        "    vectorizer = CountVectorizer()  #  Menghitung frekuensi kemunculan kata dalam teks\n",
        "    X_train_vec = vectorizer.fit_transform(X_train) # Menghitung dan menyimpan representasi vektor dari data train\n",
        "    X_test_vec = vectorizer.transform(X_test) # menghitung dan menyimpan representasi vektor dari data test\n",
        "    return X_train_vec, X_test_vec\n",
        "\n",
        "# B. Vector embedding dengan TF-IDF\n",
        "def vectorize_tfidf(X_train, X_test):\n",
        "    vectorizer = TfidfVectorizer() # Menghitung bobot TF-IDF untuk setiap kata dalam teks\n",
        "    X_train_vec = vectorizer.fit_transform(X_train) # Menghitung dan menyimpan representasi vektor dari data pelatihan\n",
        "    X_test_vec = vectorizer.transform(X_test) # Menghasilkan representasi vektor berdasarkan model yang telah dilatih sebelumnya\n",
        "    return X_train_vec, X_test_vec\n"
      ],
      "metadata": {
        "id": "jjdTjAjyyDXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 6: Evaluasi Menggunakan Akurasi dan F1-Score"
      ],
      "metadata": {
        "id": "AG7bsqzNyGkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test) # Memprediksi label berdasarkan data fitur pengujian\n",
        "    accuracy = accuracy_score(y_test, predictions) # Menghitung akurasi prediksi dengan membandingkan label sebenarnya\n",
        "    f1 = f1_score(y_test, predictions, average='weighted') # Menghitung F1 score untuk hasil prediksi dengan menggunakan metode rata-rata 'weighted', yang memperhitungkan proporsi setiap kelas\n",
        "    return accuracy, f1"
      ],
      "metadata": {
        "id": "yiB1e0dDyG1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF Embedding\n",
        "X_train_tf, X_test_tf = vectorize_tf(X_train, X_test)  # Mengubah data latih (X_train) dan data uji (X_test) menjadi representasi embedding berbasis frekuensi kata (TF)\n",
        "nb_tf = train_naive_bayes(X_train_tf, y_train)         # Melatih model Naive Bayes menggunakan data latih yang telah di-embed dengan TF\n",
        "lr_tf = train_logistic_regression(X_train_tf, y_train) # Melatih model Logistic Regression menggunakan data latih yang telah di-embed dengan TF\n",
        "\n",
        "# TF-IDF Embedding\n",
        "X_train_tfidf, X_test_tfidf = vectorize_tfidf(X_train, X_test) # Menggunakan fungsi vectorize_tfidf untuk mengubah data latih (X_train) dan data uji (X_test) menjadi representasi embedding berbasis TF-IDF\n",
        "nb_tfidf = train_naive_bayes(X_train_tfidf, y_train)           # Melatih model Logistic Regression menggunakan data latih yang telah di-embed dengan TF-IDF\n",
        "lr_tfidf = train_logistic_regression(X_train_tfidf, y_train)   # Melatih model Logistic Regression menggunakan data latih yang telah di-embed dengan TF-IDF\n",
        "\n",
        "# Evaluate models\n",
        "nb_tf_acc, nb_tf_f1 = evaluate_model(nb_tf, X_test_tf, y_test) # Menghitung akurasi dan F1 model Niave Bayes dengan TF\n",
        "lr_tf_acc, lr_tf_f1 = evaluate_model(lr_tf, X_test_tf, y_test) # Menghitung akurasi dan F1 model Logistik regression dengan TF\n",
        "\n",
        "nb_tfidf_acc, nb_tfidf_f1 = evaluate_model(nb_tfidf, X_test_tfidf, y_test) # Menghitung akurasi dan F1 model Niave Bayes dengan TF-IDF\n",
        "lr_tfidf_acc, lr_tfidf_f1 = evaluate_model(lr_tfidf, X_test_tfidf, y_test) # Menghitung akurasi dan F1 model Logistik regression dengan TF-IDF\n",
        "\n",
        "# Print Results\n",
        "print(\"Naive Bayes + TF: Accuracy =\", nb_tf_acc, \"F1 Score =\", nb_tf_f1)\n",
        "print(\"Logistic Regression + TF: Accuracy =\", lr_tf_acc, \"F1 Score =\", lr_tf_f1)\n",
        "\n",
        "print(\"Naive Bayes + TF-IDF: Accuracy =\", nb_tfidf_acc, \"F1 Score =\", nb_tfidf_f1)\n",
        "print(\"Logistic Regression + TF-IDF: Accuracy =\", lr_tfidf_acc, \"F1 Score =\", lr_tfidf_f1)\n"
      ],
      "metadata": {
        "id": "C-7mtpQZyHJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langkah 7: Perbandingan Hasil Kedua Classifier (4 model)"
      ],
      "metadata": {
        "id": "8MiPhcOAyG-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data untuk visualisasi\n",
        "models = ['Naive Bayes + TF', 'Logistic Regression + TF', 'Naive Bayes + TF-IDF', 'Logistic Regression + TF-IDF']\n",
        "accuracies = [nb_tf_acc, lr_tf_acc, nb_tfidf_acc, lr_tfidf_acc]\n",
        "f1_scores = [nb_tf_f1, lr_tf_f1, nb_tfidf_f1, lr_tfidf_f1]\n",
        "\n",
        "x = np.arange(len(models))  # label lokasi untuk model\n",
        "width = 0.35  # lebar bar\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "# Menambahkan bar untuk akurasi dan F1 score\n",
        "bars1 = ax.bar(x - width/2, accuracies, width, label='Akurasi', color='royalblue')\n",
        "bars2 = ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='lightcoral')\n",
        "\n",
        "# Menambahkan label, judul, dan legenda\n",
        "ax.set_ylabel('Skor')\n",
        "ax.set_title('Perbandingan Akurasi dan F1 Score dari Model')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "\n",
        "# Menambahkan nilai di atas setiap bar\n",
        "def add_value_labels(bars):\n",
        "    \"\"\"Menambahkan nilai di atas setiap bar.\"\"\"\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.2f}',  # format angka\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),  # koordinat\n",
        "                    xytext=(0, 3),  # offset\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "add_value_labels(bars1)\n",
        "add_value_labels(bars2)\n",
        "\n",
        "# Menampilkan plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qad5hova7mzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perbandingan hasil\n",
        "1. Model Logistic Regression dengan TF adalah yang terbaik di antara kombinasi yang diuji, dengan akurasi dan F1 score tertinggi.\n",
        "2. Naive Bayes, terutama dengan embedding TF-IDF, tidak menunjukkan performa yang baik, yang menunjukkan bahwa model ini mungkin tidak cocok untuk jenis data ini atau cara data diproses.\n",
        "3. Embedding TF lebih efektif dibandingkan dengan TF-IDF untuk kedua model yang diuji dalam hal klasifikasi review hotel. Ini mungkin menunjukkan bahwa model lebih berhasil dalam mengidentifikasi pola berdasarkan frekuensi kemunculan kata daripada dengan memperhatikan bobot kata.\n",
        "\n",
        "Kesimpulan: Model terbaik untuk klasifikasi review hotol berdasarkan pemrosesan di atas adalah Model Logistic Regression yang dikombinasikan dengan TF. Model tidak berlaku sama untuk semua kasus, sehingga tidak dapat disimpulkan model mana yang lebih baik."
      ],
      "metadata": {
        "id": "FWy1GMCm7RPN"
      }
    }
  ]
}