{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1lg5Vc3CRgrHEQiw23TQcEm2NWxXHDw-z",
      "authorship_tag": "ABX9TyM1kHM3aojrfJfWqu24ExTC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Summarization With TFIDF\n",
        "\n",
        "NIM: 215314087"
      ],
      "metadata": {
        "id": "XF-67lfcpSca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Import Library"
      ],
      "metadata": {
        "id": "6XqEpNx3In2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PySastrawi"
      ],
      "metadata": {
        "id": "AnjzJ8_gLXKA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import library\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import operator\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import defaultdict\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "Stopwords = set(stopwords.words('indonesian'))\n",
        "wordlemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "jdPTPN5TqIJm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2 - Function to clean text and calculate tf idf"
      ],
      "metadata": {
        "id": "pnzFEEAV3xWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lematization function\n",
        "def lemmatize_words(words):\n",
        "    return [wordlemmatizer.lemmatize(word) for word in words]"
      ],
      "metadata": {
        "id": "cjqaZd87kAqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove special characters function\n",
        "def remove_special_characters(text):\n",
        "    regex = r'[^a-zA-Z0-9\\s]'\n",
        "    return re.sub(regex, '', text)"
      ],
      "metadata": {
        "id": "gnWFaH4ZkCEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words frequensi function\n",
        "def freq(words):\n",
        "    words = [word.lower() for word in words]\n",
        "    dict_freq = defaultdict(int)\n",
        "    for word in words:\n",
        "        dict_freq[word] += 1\n",
        "    return dict_freq"
      ],
      "metadata": {
        "id": "KEH_Rr7lkDRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pos tagging function\n",
        "def pos_tagging(text):\n",
        "    pos_tag = nltk.pos_tag(text.split())\n",
        "    return [word for word, tag in pos_tag if tag.startswith('NN') or tag.startswith('VB')]"
      ],
      "metadata": {
        "id": "gHp1qe5dkFpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF score function\n",
        "def tf_score(word, sentence):\n",
        "    word_frequency_in_sentence = sentence.split().count(word)\n",
        "    len_sentence = len(sentence.split())\n",
        "    return word_frequency_in_sentence / len_sentence"
      ],
      "metadata": {
        "id": "QOuj0eikkHuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IDF score function\n",
        "def idf_score(no_of_sentences, word, sentences):\n",
        "    no_of_sentence_containing_word = sum(1 for sentence in sentences if word in sentence)\n",
        "    return math.log10(no_of_sentences / (no_of_sentence_containing_word + 1))"
      ],
      "metadata": {
        "id": "5VbBmJVYkKsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF function\n",
        "def tf_idf_score(tf, idf):\n",
        "    return tf * idf"
      ],
      "metadata": {
        "id": "_YPZ_uQ-kKqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tf idf of words function\n",
        "def word_tfidf(word, sentences, sentence):\n",
        "    tf = tf_score(word, sentence)\n",
        "    idf = idf_score(len(sentences), word, sentences)\n",
        "    return tf_idf_score(tf, idf)"
      ],
      "metadata": {
        "id": "Rpq7g3I6kKfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence importance function\n",
        "def sentence_importance(sentence, dict_freq, sentences):\n",
        "    sentence_score = 0\n",
        "    sentence = remove_special_characters(sentence)\n",
        "    pos_tagged_sentence = pos_tagging(sentence)\n",
        "    for word in pos_tagged_sentence:\n",
        "        if word.lower() not in Stopwords and len(word) > 1:\n",
        "            word = word.lower()\n",
        "            word = wordlemmatizer.lemmatize(word)\n",
        "            sentence_score += word_tfidf(word, sentences, sentence)\n",
        "    return sentence_score"
      ],
      "metadata": {
        "id": "ym1_8MIYkPi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Text Preprocessing"
      ],
      "metadata": {
        "id": "e_MLQXdB4BRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import file (document that want to summary) and then read\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/PI/Dataset Project PI/dev.05.txt'\n",
        "with open(file_path, 'r') as file:\n",
        "  text = file.read()"
      ],
      "metadata": {
        "id": "1hffnwtckPfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization tekxt to sentence, remove special characters and delate number from text\n",
        "tokenized_sentence = sent_tokenize(text)\n",
        "text = remove_special_characters(text)\n",
        "text = re.sub(r'\\d+', '', text)"
      ],
      "metadata": {
        "id": "VjIXV3fQkPcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords and short words then lemmatization the words\n",
        "tokenized_words_with_stopwords = word_tokenize(text)\n",
        "tokenized_words = [word.lower() for word in tokenized_words_with_stopwords if word.lower() not in Stopwords and len(word) > 1]\n",
        "tokenized_words = lemmatize_words(tokenized_words)"
      ],
      "metadata": {
        "id": "B63hG_gskZFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4 - Modelling using TF-IDF"
      ],
      "metadata": {
        "id": "HPhls77n4LeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate frequnsi of words\n",
        "word_freq = freq(tokenized_words)"
      ],
      "metadata": {
        "id": "PRRj1GQOkZCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takes input of the percentage of information to be retained from the user\n",
        "input_user = int(input('Percentage of information to retain(in percent):'))\n",
        "\n",
        "# calculate the number of sentences to be retained in the summary\n",
        "no_of_sentences = int((input_user * len(tokenized_sentence)) / 100)"
      ],
      "metadata": {
        "id": "TfP9AVzOlIl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the importance of each sentence then sorting\n",
        "sentence_with_importance = {i: sentence_importance(sent, word_freq, tokenized_sentence) for i, sent in enumerate(tokenized_sentence, 1)}\n",
        "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1), reverse=True)"
      ],
      "metadata": {
        "id": "uaMtyv_ZkY_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the sentence with the highest importance\n",
        "summary_sentences = sorted([sentence_no for sentence_no, _ in sentence_with_importance[:no_of_sentences]])\n",
        "\n",
        "# make a summary by combining important sentences\n",
        "summary = \" \".join(tokenized_sentence[i - 1] for i in summary_sentences)"
      ],
      "metadata": {
        "id": "GkX7md5XkfAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the summary of document and look the length of words\n",
        "print(\"\\nSummary:\")\n",
        "print(summary)\n",
        "len(summary)"
      ],
      "metadata": {
        "id": "Jx0XZZGMke9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn and unify separate words into sentence lines\n",
        "summary_all = ''.join([' '.join(kata) if isinstance(kata, list) else kata for kata in summary])"
      ],
      "metadata": {
        "id": "QKNZX1oboC3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(summary_all)\n",
        "len(summary_all)"
      ],
      "metadata": {
        "id": "dhn9oARpoGjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# make DataFrame from summary result\n",
        "df = pd.DataFrame([summary], columns=['Summary'])\n",
        "\n",
        "# save DataFrame with CSV/.txt  file\n",
        "df.to_csv('summary-dev.05.txt', index=False)"
      ],
      "metadata": {
        "id": "1iNXlyn8olAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Model Evaluation with ROUGE"
      ],
      "metadata": {
        "id": "zsUQ1fzRoj54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UNE6gMSypBKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "\n",
        "def evaluate_summary(generated_summary, reference_summary):\n",
        "    # Initialize the ROUGE evaluator\n",
        "    rouge = Rouge()\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    scores = rouge.get_scores(generated_summary, reference_summary, avg=True)\n",
        "\n",
        "    # Extract and print ROUGE scores\n",
        "    rouge_1 = scores['rouge-1']\n",
        "    rouge_2 = scores['rouge-2']\n",
        "    rouge_l = scores['rouge-l']\n",
        "\n",
        "    print(f\"ROUGE-1: Precision: {rouge_1['p']}, Recall: {rouge_1['r']}, F1: {rouge_1['f']}\")\n",
        "    print(f\"ROUGE-2: Precision: {rouge_2['p']}, Recall: {rouge_2['r']}, F1: {rouge_2['f']}\")\n",
        "    print(f\"ROUGE-L: Precision: {rouge_l['p']}, Recall: {rouge_l['r']}, F1: {rouge_l['f']}\")\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Example usage\n",
        "reference_summary = \"\"\"\n",
        "Menurut Cheong banyak perusahaan yang sedianya memilih Singapura sebagai kantor regional, namun memutuskan pindah ke negara lain. Ini dilakukan demi menangkap peluang-peluang yang muncuk akibat populasi generasi muda dan tumbuhnya konsumsi negara-negara tujuan investasi di Asia Tenggara.\n",
        "Salah satu contoh, yaitu Crestar Education Group, perusahaan asal Singapura yang melakukan ekspansi ke Indonesia. Manajemen Crestar menilai, potensi data beli penduduk Indonesia yang berusia relatif muda menarik untuk digarap, termasuk masyarakat berpenghasilan menengah yang jumlah banyak.\n",
        "\"\"\"\n",
        "generated_summary = \"\"\"\n",
        "Ini dilakukan demi menangkap peluang-peluang yang muncuk akibat populasi generasi muda dan tumbuhnya konsumsi negara-negara tujuan investasi di Asia Tenggara.\n",
        "Manajemen Crestar menilai, potensi data beli penduduk Indonesia yang berusia relatif muda menarik untuk digarap, termasuk masyarakat berpenghasilan menengah yang jumlah banyak.\n",
        "\"\"\"\n",
        "\n",
        "scores = evaluate_summary(generated_summary, reference_summary)\n"
      ],
      "metadata": {
        "id": "ukI3-71iqdYf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}